{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gradients with Pythorch\n",
    "\n",
    "Torch include\n",
    "\n",
    "`torch.autograd.backward()`\n",
    "\n",
    "`torch.autograd.grad()`\n",
    "\n",
    "**Autograd** provides automatic differentiation for all operations on Tensors. This is because operations become attributes of the tensor themselves. When a Tensor's `requires_grad` attribute is True, it starts to track all operations on it. When all operations finishes you can `.backward()` and have all the gradients computed automatically. The grafdients for a tensor will be accumulated into its `.grad` attribute."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Brief introduction to Back_propagation on one step\n",
    "\n",
    "A single polynomial function $y=f(x)$ to tensor $x$. Then, we'll backdrop and print the gradient $\\frac{dy}{dx}$\n",
    "\n",
    "$$\n",
    "Function: y = 2x^4 + x^3 + 3x^2 + 5x + 1\n",
    "\\\\\n",
    "Derivative: y' = 8x^3 + 3x^2 + 6x + 5\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First step. Imports\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creation of the Tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2., requires_grad=True)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tensor will requires requires_grad\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining the function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(63., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The same function as in the description\n",
    "y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1\n",
    "\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$y$ is calculated from one operation, it has an associated gradient function accessible as y.grad_fn.\n",
    "\n",
    "The value of y is 63 by this calculation.\n",
    "\n",
    "$$\n",
    "x = 2.0\n",
    "\\\\\n",
    "y = 2(2)^4 + (2)^3 + 5(2) + 1 = 32 + 8 + 12 + 10 + 1 = 63\n",
    "\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing backward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(63., grad_fn=<AddBackward0>)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check the resulting gradient of x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(93.)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As `x.grad` is an attribute of tensor $x$ we don't use parentheses. The computation is the result of the gradient.\n",
    "\n",
    "$$\n",
    "y' = 8(2)^3 + 3(2)^2 + 6(2) + 5 = 64 + 12 + 12 + 5 = 93\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Back propagation on multiple steps\n",
    "\n",
    "We now incluude layers `y` and `z` between `x` and our output layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 2., 3.],\n        [3., 2., 1.]], requires_grad=True)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a new tensor\n",
    "x = torch.tensor([[1., 2, 3], [3, 2, 1]], requires_grad=True)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First layer:\n",
    "\n",
    "$$\n",
    "y = 3x + 2\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 5.,  8., 11.],\n        [11.,  8.,  5.]], grad_fn=<AddBackward0>)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 3*x + 2\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a second layer\n",
    "\n",
    "$z = 2*y^2$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50., 128., 242.],\n",
      "        [242., 128.,  50.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# z layer\n",
    "z = 2 * y**2\n",
    "print(z)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let set the output to be a mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(140., grad_fn=<MeanBackward0>)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = z.mean()\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perform back propagation\n",
    "\n",
    "Finding the gradient of $x$ with respect to $out$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[10., 16., 22.],\n        [22., 16., 10.]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.backward()\n",
    "\n",
    "# x changes!\n",
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The way it works is calculating the partial derivative of $out$ with respect to $x_i$ as follows:\n",
    "\n",
    "Where n = 6\n",
    "\n",
    "$$\n",
    "output = \\frac{1}{n} \\sum^n_{i=1} z_i\n",
    "\\\\\n",
    "z_i = 2(y_i)^2 = 2(3x_i + 2)^2\n",
    "$$\n",
    "\n",
    "To solve the derivative of $z_i$ we use the chain rule, where the derivative of $f(g(x)) = f'(g(x))g'(x)$\n",
    "\n",
    "In this case:\n",
    "\n",
    "$$\n",
    "\\begin{split} f(g(x)) &= 2(g(x))^2, \\quad &f'(g(x)) = 4g(x)\n",
    "\\\\\n",
    "g(x) &= 3x+2, &g'(x) = 3\n",
    "\\\\\n",
    "\\frac {dz} {dx} &= 4g(x)\\times 3 &= 12(3x+2)\n",
    "\\end{split}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Therefore:\n",
    "\n",
    "$ \\frac{\\partial o}{\\partial x_i} = \\frac{1}{6} * 12(3x + 2)$\n",
    "\n",
    "$ \\frac{\\partial o}{\\partial x_i} \\bigr\\rvert_{x_i=1} = 2(3(1) + 2) = 10$\n",
    "\n",
    "$ \\frac{\\partial o}{\\partial x_i} \\bigr\\rvert_{x_i=2} = 2(3(2) + 2) = 16$\n",
    "\n",
    "$ \\frac{\\partial o}{\\partial x_i} \\bigr\\rvert_{x_i=2} = 2(3(3) + 2) = 22$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}